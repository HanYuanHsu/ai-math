
https://chat.openai.com/share/46f3ed0f-32fe-4fcf-8c6a-2dd85c586d78

You are a mathematician that is excellent at finding patterns and trends from numbers. For example, given the sequence 2, 4, 6, 8, 10, ..., you should say that sequence is increasing. Now, given 1.3, 1.5, 1.7, 1.9, what can you say

I have downloaded code llama weights in cml7. Check contents of /tmp2/hhhsu <-- cml7 gone
Go to cml9

## Run Evaluation

Take DeepSeek model for example:

1. Deploy the model

    Default run on http://localhost:8888

    ```
    cd deploy && python deepseek_api.py
    ```
2. Evaluate
    
    You can change the prompt in `prompts/prompt.py`

    ```
    python evaluate/deepseek.py
    ```

## Add models

You will complete these `python` files.

- `deploy/${NEW_MODEL}.py`: how to deploy your model to handle the prompt requests
- `prompts/prompt.py`: Add your prompt for your new models
- `models/${NEW_MODEL}.py`: how to request to your model which deployed
- `evaluate.py`: import your model and use



# Deepseek Model (The configuration used by Lewis, who scored 21)
Check out `deploy/deepseek_math_api.py`. The `if __name__ == '__main__'` block shows how
you can initialize the model and the prompt format it takes in. Remember to change the huggingface cache directory `HF_CACHE_DIR` to a directory of your choice.

To execute `deploy/deepseek_math_api.py`, go to the `ai-math` directory and run
```
python -m deploy.deepseek_math_api
```

## Utility functions used by Lewis
Check out `utils/lewis.py`. The functions there are directly copied from Lewis's notebook. The most interesting one is the `process_code` function, which executes the python code generated by the LLM. More specifically, it will write the code onto a python file `code.py` and then execute it.

Go to the `ai-math` directory and run `python -m utils.lewis` to see how the `process_code` function works.

# 6/10
Trying out finetuning.
Can use off-the shelf lora adapters: <br>
https://huggingface.co/CMU-AIR2/deepseek-math-base-LORA-ArithSteps-10k <br>

### Tutorials
https://www.kaggle.com/code/jatinsinghsagoi/aimo-24-finetune-deepseek-math <br>
^ I ran through this example, but looks like finetuning didn't make results better?<br>
Also, the notebook sets the prompt format for the model wrong. <br>
https://www.youtube.com/watch?v=Us5ZFp16PaU <br>
^ I am replicating this video in finetuning.py, but I might stop

### Strategies for improving performance
https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/discussion/505756

# 6/20
https://www.kaggle.com/code/jonathanmichala/updated-code-interpretation-492274-for-competition
https://www.kaggle.com/code/harrytheorange/updated-code-interpretation
https://www.kaggle.com/code/chewkokwahibrainai/updated-code-interpretation-n-repetitions-17
